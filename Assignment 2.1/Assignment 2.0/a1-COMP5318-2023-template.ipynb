{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: A1part2 6  , SID1: 520080414   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data as data\n",
    "# Import all libraries\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from time import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import data\n",
    "from pandas.core import frame\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (300000, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   sentiment          id                          date     query  \\\n0          0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n1          0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n2          0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n3          0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n4          0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n\n          username                                               text  \n0  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!   \n1      sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...  \n2       sammydearr  @TiannaChaos i know  just family drama. its la...  \n3      Lamb_Leanne  School email won't open  and I have geography ...  \n4      yogicerdito                             upper airways problem   ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>id</th>\n      <th>date</th>\n      <th>query</th>\n      <th>username</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2200003196</td>\n      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>LaLaLindsey0609</td>\n      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467998485</td>\n      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>sexygrneyes</td>\n      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2300048954</td>\n      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>sammydearr</td>\n      <td>@TiannaChaos i know  just family drama. its la...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1993474027</td>\n      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Lamb_Leanne</td>\n      <td>School email won't open  and I have geography ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2256550904</td>\n      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>yogicerdito</td>\n      <td>upper airways problem</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('training.300000.processed.noemoticon.csv')\n",
    "\n",
    "# # Drop 'duration' column\n",
    "# df = df.drop('date', axis=1).drop('query', axis=1)\n",
    "\n",
    "print('Shape of dataframe:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149485 150515\n",
      "CPU Time: 21.332491874694824\n"
     ]
    },
    {
     "data": {
      "text/plain": "[(['on', 'lunch', '...', 'dj', 'should', 'come', 'eat', 'with', 'me'], 1),\n (['@mrstessyman',\n   'thank',\n   'you',\n   'glad',\n   'you',\n   'like',\n   'it',\n   '!',\n   'There',\n   'is',\n   'a',\n   'product',\n   'review',\n   'bit',\n   'on',\n   'the',\n   'site',\n   'Enjoy',\n   'knitting',\n   'it',\n   '!'],\n  1),\n (['@PerezHilton',\n   'Zach',\n   'makes',\n   'me',\n   'pee',\n   'sitting',\n   'down',\n   '!',\n   'And',\n   \"I'm\",\n   'a',\n   'grown',\n   'gay',\n   'man',\n   '!'],\n  1),\n (['to',\n   'sum',\n   'up',\n   'my',\n   'day',\n   'in',\n   'one',\n   'word',\n   '...',\n   'kackered',\n   '!'],\n  1),\n (['@k9wkj', 'Great', 'minds', 'think', 'alike'], 1)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "df_tweet = df[['sentiment', 'text']]\n",
    "sen_pos = df_tweet[df_tweet['sentiment'] == 4]\n",
    "sen_neg = df_tweet[df_tweet['sentiment'] == 0]\n",
    "print(len(sen_neg), len(sen_pos))\n",
    "\n",
    "df_tweet = pd.concat([sen_pos, sen_neg])\n",
    "\n",
    "start_time = time()\n",
    "token = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df_tweet['text'].tolist()\n",
    "Y = df_tweet['sentiment'].tolist()\n",
    "\n",
    "# Building data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n",
    "# and its corresponding label\n",
    "X = df_tweet['text'].tolist()\n",
    "Y = df_tweet['sentiment'].tolist()\n",
    "\n",
    "for x, y in zip(X,Y):\n",
    "    if y == 4:\n",
    "        data.append((token.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((token.tokenize(x), 0))\n",
    "\n",
    "print('CPU Time:', time() - start_time)\n",
    "data[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'lunch', '...', 'dj', 'should', 'come', 'eat', 'with', 'me']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        # First, we will convert the pos_tag output tags to a tag format that the WordNetLemmatizer can interpret\n",
    "        # In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb.\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(data[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "# Stopwords are frequently-used words (such as “the”, “a”, “an”, “in”) that do not hold any meaning useful to extract sentiment.\n",
    "# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n",
    "# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance.\n",
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '½25':\n",
    "        return ''\n",
    "    return token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lunch', '...', 'come', 'eat']\n"
     ]
    }
   ],
   "source": [
    "# This function will be our all-in-one noise removal function\n",
    "def remove_noise(tweet_token):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_token):\n",
    "        # Eliminating the token if it is a link\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # Eliminating the token if it is a mention\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        leammatizer = WordNetLemmatizer()\n",
    "        token = leammatizer.lemmatize(token, pos)\n",
    "\n",
    "        cleaned_token = cleaned(token.lower())\n",
    "\n",
    "        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n",
    "        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Prevewing the remove_noise() output\n",
    "print(remove_noise(data[0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "#unknowns\n",
    "unks = []\n",
    "UNKS = []\n",
    "\n",
    "# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n",
    "# we have to define a function that transforms our data into the required input structure\n",
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "# Removing noise from all the dat\n",
    "for tokens, label in data:\n",
    "    cleaned_tokens_list.append((remove_noise(tokens), label))\n",
    "\n",
    "print('Removed Noise, CPU Time:', time() - start_time)\n",
    "start_time = time()\n",
    "\n",
    "final_data = []\n",
    "\n",
    "# Transforming the data to fit the input structure of the Naive Bayesian classifier\n",
    "for tokens, label in cleaned_tokens_list:\n",
    "    final_data.append((list_to_dict(tokens),label))\n",
    "\n",
    "print('Data Prepared for model, CPU Time:', time() - start_time)\n",
    "\n",
    "# Previewing our final (tokenized, cleaned and lemmatized) data list\n",
    "final_data[:5]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cleared(word):\n",
    "    res = ''\n",
    "    prev = None\n",
    "    for char in word:\n",
    "        if char == prev: continue\n",
    "        prev = char\n",
    "        res += char\n",
    "    return res\n",
    "\n",
    "def sentence_to_indices(sentence_words, word_to_index, max_len, i):\n",
    "    global X, Y\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            UNKS.append(w)\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "                unks.append(w)\n",
    "        X[i, j] = index\n",
    "\n",
    "list_len = [len(i) for i, j in cleaned_tokens_list]\n",
    "max_len = max(list_len)\n",
    "X = np.zeros((len(cleaned_tokens_list), max_len))\n",
    "Y = np.zeros((len(cleaned_tokens_list), ))\n",
    "\n",
    "# for i, tk_lb in enumerate(cleaned_tokens_list):\n",
    "#     tokens, label = tk_lb\n",
    "#     # sentence_to_indices(tokens, word_to_index, max_len, i)\n",
    "#     Y[i] = label\n",
    "\n",
    "print(max_len)\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# As data is currently ordered by label, we have to shuffle it before splitting it\n",
    "# .Random(140) randomizes our data with seed = 140. This guarantees the same shuffling for every execution of our code\n",
    "random.Random(140).shuffle(final_data)\n",
    "\n",
    "# 90% train data and 10% test data\n",
    "trim_index = int(len(final_data) * 0.8)\n",
    "\n",
    "train_data = final_data[:trim_index]\n",
    "test_data = final_data[trim_index:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy on train data: 0.8192583333333333\n",
      "Naive Bayes Accuracy on test data: 0.7487333333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 20\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNaive Bayes Accuracy on test data:\u001B[39m\u001B[38;5;124m'\u001B[39m, classify\u001B[38;5;241m.\u001B[39maccuracy(naive, test_data))\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# sklearn = SklearnClassifier.train(train_data,)\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# print('Naive Bayes Accuracy on train data:', sklearn.accuracy(naive, train_data))\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# print('Naive Bayes Accuracy on test data:', sklearn.accuracy(naive, test_data))\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m decision \u001B[38;5;241m=\u001B[39m \u001B[43mDecisionTreeClassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDecision Tree Accuracy on train data:\u001B[39m\u001B[38;5;124m'\u001B[39m, classify\u001B[38;5;241m.\u001B[39maccuracy(decision, train_data))\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDecisionT ree Accuracy on test data:\u001B[39m\u001B[38;5;124m'\u001B[39m, classify\u001B[38;5;241m.\u001B[39maccuracy(decision, test_data))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\classify\\decisiontree.py:166\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.train\u001B[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;66;03m# Start with a stump.\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m binary:\n\u001B[1;32m--> 166\u001B[0m     tree \u001B[38;5;241m=\u001B[39m \u001B[43mDecisionTreeClassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_stump\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabeled_featuresets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    170\u001B[0m     tree \u001B[38;5;241m=\u001B[39m DecisionTreeClassifier\u001B[38;5;241m.\u001B[39mbest_binary_stump(\n\u001B[0;32m    171\u001B[0m         feature_names, labeled_featuresets, feature_values, verbose\n\u001B[0;32m    172\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\classify\\decisiontree.py:263\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.best_stump\u001B[1;34m(feature_names, labeled_featuresets, verbose)\u001B[0m\n\u001B[0;32m    261\u001B[0m best_error \u001B[38;5;241m=\u001B[39m best_stump\u001B[38;5;241m.\u001B[39merror(labeled_featuresets)\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fname \u001B[38;5;129;01min\u001B[39;00m feature_names:\n\u001B[1;32m--> 263\u001B[0m     stump \u001B[38;5;241m=\u001B[39m \u001B[43mDecisionTreeClassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabeled_featuresets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    264\u001B[0m     stump_error \u001B[38;5;241m=\u001B[39m stump\u001B[38;5;241m.\u001B[39merror(labeled_featuresets)\n\u001B[0;32m    265\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stump_error \u001B[38;5;241m<\u001B[39m best_error:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\classify\\decisiontree.py:201\u001B[0m, in \u001B[0;36mDecisionTreeClassifier.stump\u001B[1;34m(feature_name, labeled_featuresets)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m featureset, label \u001B[38;5;129;01min\u001B[39;00m labeled_featuresets:\n\u001B[0;32m    200\u001B[0m     feature_value \u001B[38;5;241m=\u001B[39m featureset\u001B[38;5;241m.\u001B[39mget(feature_name)\n\u001B[1;32m--> 201\u001B[0m     \u001B[43mfreqs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature_value\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    203\u001B[0m decisions \u001B[38;5;241m=\u001B[39m {val: DecisionTreeClassifier(freqs[val]\u001B[38;5;241m.\u001B[39mmax()) \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m freqs}\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DecisionTreeClassifier(label, feature_name, decisions)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\probability.py:121\u001B[0m, in \u001B[0;36mFreqDist.__setitem__\u001B[1;34m(self, key, val)\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_N \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_N\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__setitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, val):\n\u001B[0;32m    122\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;124;03m    Override ``Counter.__setitem__()`` to invalidate the cached N\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_N \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import DecisionTreeClassifier\n",
    "from nltk import SklearnClassifier\n",
    "from nltk import MaxentClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "naive = NaiveBayesClassifier.train(train_data)\n",
    "print('Naive Bayes Accuracy on train data:', classify.accuracy(naive, train_data))\n",
    "print('Naive Bayes Accuracy on test data:', classify.accuracy(naive, test_data))\n",
    "\n",
    "# sklearn = SklearnClassifier.train(train_data,)\n",
    "# print('Naive Bayes Accuracy on train data:', sklearn.accuracy(naive, train_data))\n",
    "# print('Naive Bayes Accuracy on test data:', sklearn.accuracy(naive, test_data))\n",
    "\n",
    "decision = DecisionTreeClassifier.train(train_data)\n",
    "print('Decision Tree Accuracy on train data:', classify.accuracy(decision, train_data))\n",
    "print('DecisionT ree Accuracy on test data:', classify.accuracy(decision, test_data))\n",
    "\n",
    "# maxent = MaxentClassifier.train(train_data)\n",
    "# print('Maxent Accuracy on train data:', classify.accuracy(maxent, train_data))\n",
    "# print('Maxent Accuracy on test data:', classify.accuracy(maxent, test_data))\n",
    "\n",
    "print('\\nCPU Time:', time() - start_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Label Encoder\n",
    "labels = train_data.target.unique().tolist()\n",
    "labels.append(\"NEUTRAL\")\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "\n",
    "x_train = pad_sequences\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data.target.tolist())\n",
    "\n",
    "y_train = encoder.transform(train_data.target.tolist())\n",
    "y_test = encoder.transform(test_data.target.tolist())\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature: (240000, 5)\n",
      "Shape of testing feature: (60000, 5)\n",
      "Shape of training label: (240000,)\n",
      "Shape of training label: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "# dataframe = df.replace('?', np.nan).replace('class1', 0).replace('class2', 1)\n",
    "# simputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# minmaxscaler = preprocessing.MinMaxScaler()\n",
    "# dataset = imputer.fit_transform(dataframe)\n",
    "# dataset = minmaxscaler.fit_transform(dataset)\n",
    "\n",
    "# Select Features\n",
    "feature = df_tweet.drop('sentiment', axis=1)\n",
    "\n",
    "# Select Target\n",
    "target = df_tweet['sentiment']\n",
    "\n",
    "# Set Training and Testing Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature , target,\n",
    "                                                    shuffle = True,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "\n",
    "# Show the Training and Testing Data\n",
    "print('Shape of training feature:', X_train.shape)\n",
    "print('Shape of testing feature:', X_test.shape)\n",
    "print('Shape of training label:', y_train.shape)\n",
    "print('Shape of training label:', y_test.shape)\n",
    "\n",
    "def print_data(X, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "\n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num][0:-1:1]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "        print(int(X[example_num][-1]))\n",
    "\n",
    "\n",
    "# print_data(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    logreg = LogisticRegression(random_state=0)\n",
    "    scores = cross_val_score(logreg, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    nb = GaussianNB()\n",
    "    scores = cross_val_score(nb, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    scores = cross_val_score(classifier, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    classifier = BaggingClassifier(DecisionTreeClassifier(max_depth=max_depth, criterion='entropy', random_state=0)\n",
    "                                   , n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    scores = cross_val_score(classifier, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth=max_depth, criterion='entropy', random_state=0)\n",
    "                                    , n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(classifier, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    classifier = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(classifier, np.asarray(X, dtype='float64'), y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Sun May 31 04:56:09 PDT 2009'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[87], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m gb_learning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Print results for each classifier in part 1 to 4 decimal places here:\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogR average cross-validation accuracy: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[43mlogregClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNB average cross-validation accuracy: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(nbClassifier(X_train, y_train)))\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDT average cross-validation accuracy: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(dtClassifier(X_train, y_train)))\n",
      "Cell \u001B[1;32mIn[83], line 4\u001B[0m, in \u001B[0;36mlogregClassifier\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlogregClassifier\u001B[39m(X, y):\n\u001B[0;32m      3\u001B[0m     logreg \u001B[38;5;241m=\u001B[39m LogisticRegression(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m     scores \u001B[38;5;241m=\u001B[39m cross_val_score(logreg, \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfloat64\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m, y, cv\u001B[38;5;241m=\u001B[39mcvKFold)\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m scores\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:2070\u001B[0m, in \u001B[0;36mNDFrame.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m   2069\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype: npt\u001B[38;5;241m.\u001B[39mDTypeLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m-> 2070\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Sun May 31 04:56:09 PDT 2009'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Parameters for Part 1:\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(X_train, y_train)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(X_train, y_train)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(X_train, y_train)))\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(\n",
    "    bagDTClassifier(X_train, y_train, bag_n_estimators, bag_max_samples, bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(\n",
    "    adaDTClassifier(X_train, y_train, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(X_train, y_train, gb_n_estimators, gb_learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    classifier = KNeighborsClassifier()\n",
    "    grid_search= GridSearchCV(classifier, param_grid, cv=cvKFold, return_train_score=True)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5, 15]\n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', random_state=0), param_grid, cv=cvKFold)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=0, criterion='entropy'), param_grid, cv=cvKFold,\n",
    "                               return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k:  1\n",
      "KNN best p:  1\n",
      "KNN cross-validation accuracy: 1.0000\n",
      "KNN test set accuracy: 1.0000\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py\", line 201, in fit\n    y = self._validate_targets(y)\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py\", line 749, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKNN test set accuracy: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(bestKNN\u001B[38;5;241m.\u001B[39mscore(X_test, y_test)))\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n\u001B[1;32m---> 16\u001B[0m bestSVM \u001B[38;5;241m=\u001B[39m \u001B[43mbestSVMClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSVM best C: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(bestSVM\u001B[38;5;241m.\u001B[39mbest_params_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSVM best gamma: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(bestSVM\u001B[38;5;241m.\u001B[39mbest_params_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n",
      "Cell \u001B[1;32mIn[34], line 10\u001B[0m, in \u001B[0;36mbestSVMClassifier\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m      8\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m: C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m: gamma}\n\u001B[0;32m      9\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m), param_grid, cv\u001B[38;5;241m=\u001B[39mcvKFold)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m grid_search\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    868\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    869\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    870\u001B[0m     )\n\u001B[0;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 874\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    878\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1386\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1387\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1388\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:851\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    844\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m!=\u001B[39m n_candidates \u001B[38;5;241m*\u001B[39m n_splits:\n\u001B[0;32m    845\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    846\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv.split and cv.get_n_splits returned \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    847\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minconsistent results. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    848\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_splits, \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m n_candidates)\n\u001B[0;32m    849\u001B[0m     )\n\u001B[1;32m--> 851\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[38;5;66;03m# For callable self.scoring, the return type is only know after\u001B[39;00m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001B[39;00m\n\u001B[0;32m    855\u001B[0m \u001B[38;5;66;03m# can now be inserted with the correct key. The type checking\u001B[39;00m\n\u001B[0;32m    856\u001B[0m \u001B[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscoring):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[1;34m(results, error_score)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[0;32m    361\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    363\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    364\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    365\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    366\u001B[0m     )\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    370\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    371\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    372\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    377\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: \nAll the 250 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n250 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py\", line 201, in fit\n    y = self._validate_targets(y)\n  File \"C:\\Users\\FU\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py\", line 749, in _validate_targets\n    raise ValueError(\nValueError: The number of classes has to be greater than one; got 1 class\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn).\n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "bestKNN = bestKNNClassifier(X_train, y_train)\n",
    "print(\"KNN best k: \", bestKNN.best_params_['n_neighbors'])\n",
    "print(\"KNN best p: \", bestKNN.best_params_['p'])\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(bestKNN.best_score_))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(bestKNN.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "bestSVM = bestSVMClassifier(X_train, y_train)\n",
    "print(\"SVM best C: {:.4f}\".format(bestSVM.best_params_['C']))\n",
    "print(\"SVM best gamma: {:.4f}\".format(bestSVM.best_params_['gamma']))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(bestSVM.best_score_))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(bestSVM.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "bestRFC = bestRFClassifier(X_train, y_train)\n",
    "y_predict = bestRFC.predict(X_test)\n",
    "print(\"RF best n_estimators: \", bestRFC.best_params_['n_estimators'])\n",
    "print(\"RF best max_leaf_nodes: \", bestRFC.best_params_['max_leaf_nodes'])\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(bestRFC.best_score_))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(bestRFC.score(X_test, y_test)))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(f1_score(y_test, y_predict, average='macro')))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(f1_score(y_test, y_predict, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
